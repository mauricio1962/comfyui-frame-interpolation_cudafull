#!/usr/bin/env python3
"""
GPUä¼˜åŒ–æµ‹è¯•è„šæœ¬
éªŒè¯RIFEæ’å¸§æ˜¯å¦å®Œå…¨åœ¨GPUä¸Šè¿è¡Œ
"""

import torch
import time
import psutil
import os

def test_gpu_optimization():
    print("=== ComfyUI Frame Interpolation GPUä¼˜åŒ–æµ‹è¯• ===")
    
    # æ£€æŸ¥CUDAçŠ¶æ€
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUæµ‹è¯•")
        return False
    
    print(f"âœ… CUDAå¯ç”¨: {torch.cuda.get_device_name()}")
    print(f"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # æ£€æŸ¥GPUå†…å­˜
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"âœ… GPUå†…å­˜: {gpu_memory:.1f} GB")
    
    # æµ‹è¯•GPUå†…å­˜åˆ†é…
    try:
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        test_tensor = torch.randn(4, 3, 512, 512, device='cuda', dtype=torch.float16)
        print(f"âœ… GPUå¼ é‡åˆ›å»ºæˆåŠŸ: {test_tensor.shape}")
        
        # æ£€æŸ¥æ˜¯å¦åœ¨GPUä¸Š
        assert test_tensor.is_cuda, "å¼ é‡ä¸åœ¨GPUä¸Š!"
        print("âœ… å¼ é‡ç¡®è®¤åœ¨GPUä¸Š")
        
        # æµ‹è¯•GPUè®¡ç®—
        start_time = time.time()
        result = torch.nn.functional.interpolate(test_tensor, scale_factor=2.0, mode='bilinear')
        torch.cuda.synchronize()  # ç­‰å¾…GPUè®¡ç®—å®Œæˆ
        gpu_time = time.time() - start_time
        
        print(f"âœ… GPUè®¡ç®—æµ‹è¯•å®Œæˆ: {gpu_time:.4f}s")
        print(f"âœ… è¾“å‡ºå½¢çŠ¶: {result.shape}")
        print(f"âœ… è¾“å‡ºè®¾å¤‡: {result.device}")
        
        # æ¸…ç†
        del test_tensor, result
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def monitor_gpu_usage():
    """ç›‘æ§GPUä½¿ç”¨ç‡"""
    print("\n=== GPUä½¿ç”¨ç‡ç›‘æ§ ===")
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            gpu_util, mem_used, mem_total = result.stdout.strip().split(', ')
            print(f"GPUåˆ©ç”¨ç‡: {gpu_util}%")
            print(f"GPUå†…å­˜ä½¿ç”¨: {mem_used}MB / {mem_total}MB ({float(mem_used)/float(mem_total)*100:.1f}%)")
        else:
            print("âš ï¸ æ— æ³•è·å–GPUä½¿ç”¨ç‡ä¿¡æ¯")
    except Exception as e:
        print(f"âš ï¸ GPUç›‘æ§å¤±è´¥: {e}")

def test_vfi_gpu_mode():
    """æµ‹è¯•VFI GPUæ¨¡å¼"""
    print("\n=== VFI GPUæ¨¡å¼æµ‹è¯• ===")
    try:
        # å¯¼å…¥VFIå·¥å…·
        from vfi_utils import GPU_ONLY_MODE, AGGRESSIVE_GPU_OPTIMIZATION, preprocess_frames, postprocess_frames
        
        print(f"GPU_ONLY_MODE: {GPU_ONLY_MODE}")
        print(f"AGGRESSIVE_GPU_OPTIMIZATION: {AGGRESSIVE_GPU_OPTIMIZATION}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿå¸§æ•°æ®
        frames = torch.randn(2, 512, 512, 3)  # 2å¸§ï¼Œ512x512ï¼ŒRGB
        print(f"åŸå§‹å¸§æ•°æ®: {frames.shape}, è®¾å¤‡: {frames.device}")
        
        # æµ‹è¯•é¢„å¤„ç†
        processed = preprocess_frames(frames)
        print(f"é¢„å¤„ç†å: {processed.shape}, è®¾å¤‡: {processed.device}")
        
        # æµ‹è¯•åå¤„ç†ï¼ˆä¿æŒåœ¨GPUï¼‰
        result = postprocess_frames(processed, keep_on_gpu=True)
        print(f"åå¤„ç†å(GPU): {result.shape}, è®¾å¤‡: {result.device}")
        
        # æµ‹è¯•åå¤„ç†ï¼ˆè½¬åˆ°CPUï¼‰
        result_cpu = postprocess_frames(processed, keep_on_gpu=False)
        print(f"åå¤„ç†å(CPU): {result_cpu.shape}, è®¾å¤‡: {result_cpu.device}")
        
        print("âœ… VFI GPUæ¨¡å¼æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ VFI GPUæ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = True
    
    # åŸºç¡€GPUæµ‹è¯•
    success &= test_gpu_optimization()
    
    # GPUä½¿ç”¨ç‡ç›‘æ§
    monitor_gpu_usage()
    
    # VFI GPUæ¨¡å¼æµ‹è¯•
    success &= test_vfi_gpu_mode()
    
    print(f"\n=== æµ‹è¯•æ€»ç»“ ===")
    if success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! GPUä¼˜åŒ–å·²å¯ç”¨")
        print("ğŸ’¡ å»ºè®®:")
        print("  - ä½¿ç”¨è¾ƒå¤§çš„batch sizeå……åˆ†åˆ©ç”¨GPU")
        print("  - ç›‘æ§GPUåˆ©ç”¨ç‡ç¡®ä¿è¾¾åˆ°80%+")
        print("  - å¦‚é‡åˆ°å†…å­˜ä¸è¶³ï¼Œé€‚å½“é™ä½clear_cache_after_n_frames")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    
    print("\nè¿è¡Œnvidia-smiç›‘æ§GPUçŠ¶æ€:")
    monitor_gpu_usage() 